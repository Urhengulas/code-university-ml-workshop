{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from helper_functions import plot_supervised_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part we'll look at support vector machines and how it performs compared with the previous model we trained (logistic regression) for the same number of features and the same train/test split. \n",
    "\n",
    "At the end, we'll introduce and discuss a concept called cross validation, which allows us to better evaluate our model and avoid a common problem known as model overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we load the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iris_dataset = load_iris()\n",
    "X = iris_dataset.data\n",
    "y = iris_dataset.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Machines is a complicated title for a fairly simple concept.\n",
    "\n",
    "It comes down to finding a hyperplane which separates our items into two classes (hyperplane as opposed to line/curve because we can work in a number of dimensions - the number of dimensions depends on the number of features). Look below for how we use this to classify more than 2 classes.\n",
    "\n",
    "The kernel (or function) specifies which shape of hyperplane is used. For our Iris data set, we used a linear kernel, which means we are linearly separating our items for classification.\n",
    "\n",
    "---\n",
    "\n",
    "This might help you envision this on a 2-dimensional level:\n",
    "\n",
    "Imagine an X-Y plane, with a bunch of dots representing data points scattered across the plan. In this X-Y plane, we only have dots which belong to one of two classes. We want to separate the dots by their class and we plan to use a line to do that. On one side of the line, we should have all the dots representing one class and on the other we should have the other class.\n",
    "\n",
    "Small problem, though, our data isn't that simple to split. We can't find a line which puts all dots from one class on one side and all from another class on the other side. Some of our dots are mixed in with dots from the other class.\n",
    "\n",
    "That's fine. Our model just tries to do its best and pick the best line which breaks up the dots as best as possible into two classes. Some dots will be misclassified, but as long as most aren't, we're still doing well. We also want to make sure that line is generalizable enough.\n",
    "\n",
    "---\n",
    "Okay, sounds great for separating items into two classes, but what happens if you have multiple classes, like we do in our iris example?\n",
    "\n",
    "SVMs, like traditional Logisitic Regression models, are meant for binary classification, but they can also be used in multiclass cases.\n",
    "\n",
    "We again use \"one-versus-the-rest\" to see which class has the greatest margin classification for the item. Which class the item seems to best fit into. Then we return that as the item's class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = SVC(kernel='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the first two features (sepal length and width)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_sub = X[:, :2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separate the data into training and test sets using a 80/20 split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_sub, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model and predict the labels for this new model.<br />\n",
    "\n",
    "(Note: All of our code so far, except `model = SVC(kernel='linear')` has been identical to the code for Logistic Regression. That's the great thing about using `scikit-learn'. It is easy to swap out models, so you can focus on what the right model is, as opposed to the implementation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the accuracy of this new model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_supervised_model('SVM, 2 features, 20% test set', model, X_test, y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the accuracy of the support vector machine classifier with 3 features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO - train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO - train\n",
    "# TODO - predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO - accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TODO - plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does support vector machine model do compared to logistic regression for the same number of features and train/test split?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Overfitting and Cross Validation\n",
    "\n",
    "There's a common problem that happens when building models. It's called _overfitting_.\n",
    "\n",
    "The goal of a model is to learn generalizations from a training set of data and use these generalizations to predict future unknown examples.\n",
    "\n",
    "Our generalizations are only as good as the diversity of our training set/previous examples. Overfitting happens when you learn too much from a specific set of examples, in a way that makes them worse generalizations for unseen data.\n",
    "\n",
    "This can easily happen if we always use the same training and test sets. We can get really good at learning exactly the right coefficients and parameters for our models, so that we always do well on our test set. Then when we get unseen data in the future, we perform terribly, because the data are not similar to the test set we worked with.\n",
    "\n",
    "This is where cross validation can come in.\n",
    "\n",
    "With cross validation, we use the same data, but work with several train/test splits of that data. We average the accuracy across all runs with these different train/test splits and use that as the accuracy of our model. By using different splits, we get a more diverse set of test sets and it is more likely that our learned model can handle more diverse examples in the future.\n",
    "\n",
    "Cross validation allows us to make more use of the data we have, and create better generalizing models, without having to collect significantly more data (though of course, the more examples you have, the better!)\n",
    "\n",
    "We use the `cross_val_score` to compute the cross-validation accuracy score of model. The `cv` parameter determines cross-validation splitting strategy. For an integer (N), it splits the data into N training and test sets and determines the accuracy score for each of them. N can be as large as the number of your samples; when N is equal to the number of samples we call that _leave one out_. In each step of leave one out cross validation we only have 1 data point in our test set, and all but one data points in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cross_val_score(model, X_sub, y, cv=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
