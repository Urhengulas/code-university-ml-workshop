{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'plot_unsupervised_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-e1b25fc24fd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msilhouette_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhelper_functions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot_unsupervised_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'plot_unsupervised_model'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from helper_functions import plot_unsupervised_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, in the last part we'll look at one unsupervised machine learning model, K-Means Clustering, and discuss the method for determining the optimal number of clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "titanic = pd.read_csv('titanic_processed.csv')\n",
    "X = titanic.drop('survived', axis = 1)\n",
    "y = titanic['survived']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### More about Unsupervised Learning and K-Means Clustering\n",
    "\n",
    "**Unsupervised Learning**\n",
    "\n",
    "This is the first unsupervised model we have seen in this workshop.\n",
    "Unsupervised means that we do not get the benefit of labeled data. We have a lot of data, but we don't know what classes they belong to. This is very common in the world.\n",
    "\n",
    "Often, we don't have access to labeled data for our examples or we don't have the time or resources to label all of the data within a massive data set. Sometimes it's also unclear which label should apply. Think about classifying music into genres for example. There is such a diversity in music that it's difficult to find a representative set of genres.\n",
    "\n",
    "If we pick a small subset of genres, we might find it really hard to pick a genre for a given song or we might find that a song fits into multiple genres. It might be better just to make groups based on similarities, rather than start with classes from the beginning.\n",
    "\n",
    "**K-means Clustering**\n",
    "\n",
    "K-means clustering is one method of unsupervised learning.\n",
    "\n",
    "It is an _iterative_ algorithm that helps classify data points into `k` different clusters. We can then apply labels to these clusters if we like and use these labeled sets in a supervised method. Or, we can just keep these clusters and assign new data to the clusters and make recommendations or predictions based on that.\n",
    "\n",
    "Round One\n",
    "\n",
    "1. K-means clustering begins by picking `k` points as random to serve as the center of the `k` clusters we want to build.\n",
    "The `k` points serve as the centers (centroids) of their own clusters.\n",
    "\n",
    "2. Let's call our total number of data points `n`. We then look at the other `n-k` data points and assign them to the cluster that they are closest to. That is, we find the centroid which is closest to the data point, and assign that point to the cluster with that centroid.\n",
    "\n",
    "3. After all of this, we compute the new centers of each of these clusters. We get that by finding the averages of each axis (index) in the feature vector. Now we have `k` new centroids.\n",
    "\n",
    "This is the end of round one.\n",
    "\n",
    "For the next round, we do the same process, but with our new `k` centroids.\n",
    "\n",
    "1. Assign all of the `n-k` other data points into the nearest cluster.\n",
    "2. Recompute the centroids of these clusters.\n",
    "3. Proceed to the next round.\n",
    "\n",
    "We keep running these rounds until our centroids are no longer changing. Either that, or until we hit a limit of runs we determined beforehand.\n",
    "\n",
    "At the end of all of this, we return the `k` clusters that we have determined. These should be quite stable and good for predicting/assigning new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll initialize three different types of models. We will be able to see how the number of clusters affects the performance of our model.\n",
    "\n",
    "Before running the code, what is your hypothesis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = [('KMeans 2 clusters', KMeans(n_clusters=2)),\n",
    "          ('KMeans 3 clusters', KMeans(n_clusters=3)), \n",
    "          ('KMeans 8 clusters', KMeans(n_clusters=8))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the models look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_unsupervised_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-1bfe5d7ff4c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mplot_unsupervised_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_unsupervised_model' is not defined"
     ]
    }
   ],
   "source": [
    "for name, model in models:\n",
    "    plot_unsupervised_model(name, model, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a way to determine how many clusters to choose. It's not always clearcut, but there are a few scores we can use to help us.\n",
    "\n",
    "We will look at the silhouette score. Another popular way of finding the optimal number of clusters is the elbow method. You can read more about it [here](https://datasciencelab.wordpress.com/2013/12/27/finding-the-k-in-k-means-clustering/) \n",
    "\n",
    "Silhouette score says that our clusters are optimal (silhouette score is large) when intra-cluster points are as close together as possible and the distance between clusters is as large as possible. This makes sense as a useful metric.\n",
    "\n",
    "If we have densely packed clusters which are very far from one another, we can feel confident that we have separated the data into meaningful clusters.\n",
    "\n",
    "If the clusters are close to another another and/or the points within a cluster are not that close, we can't feel as confident in our classifications. Maybe our result was dependent on the specific random points we started with. Maybe we're exaggerating very small and insignificant feature differences between some data points.\n",
    "\n",
    "_Play around with the number of clusters and see how the silhouette score changes._<br />\n",
    "_Which cluster number produced the best silhouette score?_<br />\n",
    "_Why do you think that was the case_?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for n_cluster in range(2, 9):\n",
    "    model = KMeans(n_clusters=n_cluster).fit(X)\n",
    "    silhouette_coefficient = silhouette_score(X, model.labels_, metric='euclidean')\n",
    "    print(\"{} clusters: {}\".format(n_cluster, silhouette_coefficient))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
